#cloud-config

write_files:

  - path: /etc/spark/conf/log4j.properties
    content: ${log4j_properties}

  - path: /etc/spark/conf/spark-env.sh
    content: ${spark_env_sh}

  - path: /etc/spark/conf/spark-defaults.conf
    content: ${spark_defaults_conf}

  - path: /etc/spark/start.sh
    permissions: 0755
    content: |
      #!/bin/sh

      export AWS_ACCESS_KEY_ID=${aws_access_key_id}
      export AWS_SECRET_ACCESS_KEY=${aws_secret_access_key}

      aws ecr get-login-password --region us-east-1 | \
        docker login --username AWS --password-stdin ${ecr_server}

      docker run -d \
        --name spark \
        --network host \
        --env SPARK_ENV=prod \
        -v /data:/data \
        -v /etc/spark/conf:/opt/spark/conf \
        -p 8080:8080 \
        ${!master && gpu_workers ? "--gpus all" : ""} \
        ${ecr_server}/${ecr_repo} \
        ${
          master ?
          "spark-class org.apache.spark.deploy.master.Master" :
          "spark-class org.apache.spark.deploy.worker.Worker spark://${master_private_ip}:7077"
        }

  - path: /home/ubuntu/spark-bash.sh
    permissions: 0755
    content: |
      #!/bin/bash

      CONTAINER_ID=$(sudo docker ps -q --filter "name=spark")
      DOCKER_CMD="sudo docker exec -it $${CONTAINER_ID} bash"

      tmux attach -t spark || tmux new -s spark "$DOCKER_CMD"

runcmd:
  - sh /etc/spark/start.sh