---

- name: Wait for SSH connection
  wait_for_connection:

- name: Gather facts, after SSH
  setup:

# packer
- name: Install docker-py
  pip:
    name: docker

# packer
- name: Create directories
  file:
    path: '{{ item }}'
    state: directory
  with_items:
    - '{{ spark_config_dir }}'
    - '{{ spark_data_root }}'

- name: Render Spark config
  template:
    src: '{{ item }}.j2'
    dest: '{{ spark_config_dir }}/{{ item }}'
  with_items:
    - spark-defaults.conf
    - spark-env.sh
    - log4j.properties

# packer
- name: Render Docker bash script
  template:
    src: docker-bash.sh.j2
    dest: /home/{{ ansible_user }}/docker-bash.sh
    owner: '{{ ansible_user }}'
    mode: u+x

# packer
- name: Automatically connect to container on login
  lineinfile:
    path: /home/{{ ansible_user }}/.bashrc
    line: source ./docker-bash.sh

# TODO: docker_login?
- name: Login to AWS ECR
  shell: >
    $(aws ecr get-login --no-include-email
    --region {{ spark_ecr_region }})
  environment:
    AWS_ACCESS_KEY_ID: '{{ spark_aws_access_key_id }}'
    AWS_SECRET_ACCESS_KEY: '{{ spark_aws_secret_access_key }}'

- name: Start master
  include_tasks: start_container.yml
  when: '"master" in group_names'
  vars:
    name: spark-master
    runtime: '{{ spark_master_docker_runtime }}'
    command: spark-class org.apache.spark.deploy.master.Master

- name: Start workers
  include_tasks: start_container.yml
  when: '"workers" in group_names'
  vars:
    name: spark-worker
    runtime: '{{ spark_worker_docker_runtime }}'
    command: >
      spark-class org.apache.spark.deploy.worker.Worker
      {{ spark_master_url }}
