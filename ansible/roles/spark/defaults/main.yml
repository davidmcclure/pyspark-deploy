---

ansible_python_interpreter: python3

spark_docker_image: dclure/spark

spark_home: /opt/spark

spark_master_port: 7077
spark_master_url: spark://{{ tf_master_private_ip }}:{{ spark_master_port }}

spark_packages:
  - org.apache.hadoop:hadoop-aws:2.7.3
  - org.apache.spark:spark-avro_2.11:2.4.0

spark_task_max_failures: 20
spark_s3a_connection_maximum: 2000

spark_scratch_root: /data
spark_local_dir: '{{ spark_scratch_root }}/spark'
spark_warehouse_dir: '{{ spark_scratch_root }}/spark-warehouse'
spark_derby_dir: '{{ spark_scratch_root }}/derby'
spark_hadoop_tmp_dir: '{{ spark_scratch_root }}/hadoop'
spark_worker_dir: '{{ spark_scratch_root }}/work'
spark_config_dir: /etc/spark

spark_volumes:
  - '{{ spark_config_dir }}:{{ spark_home }}/conf'

# Use EC2 public DNS.
spark_public_dns: >
  `wget -q -O - http://169.254.169.254/latest/meta-data/public-hostname ||
   wget -q -O - http://169.254.169.254/latest/meta-data/local-ipv4`

spark_max_files: 100000

spark_docker_env:
  AWS_ACCESS_KEY_ID: '{{ aws_access_key_id }}'
  AWS_SECRET_ACCESS_KEY: '{{ aws_secret_access_key }}'
  SPARK_ENV: prod

spark_openblas_num_threads: 1

spark_ecr_login: false
spark_ecr_region: us-east-1

# Provide locally.
aws_access_key_id: null
aws_secret_access_key: null
