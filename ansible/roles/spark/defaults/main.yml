---

spark_home: /opt/spark

spark_master_port: 7077
spark_master_url: spark://{{ tf_master_private_ip }}:{{ spark_master_port }}

spark_base_packages:
  - org.apache.spark:hadoop-cloud_2.12:3.0.2

spark_extra_packages: []

spark_packages: '{{ spark_base_packages + spark_extra_packages }}'

spark_task_max_failures: 20
spark_s3a_connection_maximum: 2000

spark_data_root: /data
spark_local_dir: '{{ spark_data_root }}/spark'
spark_warehouse_dir: '{{ spark_data_root }}/spark-warehouse'
spark_derby_dir: '{{ spark_data_root }}/derby'
spark_hadoop_tmp_dir: '{{ spark_data_root }}/hadoop'
spark_worker_dir: '{{ spark_data_root }}/work'
spark_config_dir: /etc/spark

spark_volumes:
  - '{{ spark_config_dir }}:{{ spark_home }}/conf'
  - '{{ spark_data_root }}:{{ spark_data_root }}'

# Use EC2 public DNS.
spark_public_dns: >
  `wget -q -O - http://169.254.169.254/latest/meta-data/public-hostname ||
   wget -q -O - http://169.254.169.254/latest/meta-data/local-ipv4`

spark_max_files: 100000

spark_docker_env:
  AWS_ACCESS_KEY_ID: '{{ spark_aws_access_key_id }}'
  AWS_SECRET_ACCESS_KEY: '{{ spark_aws_secret_access_key }}'
  SPARK_ENV: prod

spark_openblas_num_threads: 1

spark_ecr_region: us-east-1

# Required from config

# spark_docker_image:
# spark_aws_access_key_id:
# spark_aws_secret_access_key:
# spark_driver_memory:
# spark_executor_memory:
# spark_driver_max_result_size:
# spark_master_docker_runtime:
# spark_worker_docker_runtime: