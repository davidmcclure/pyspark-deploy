---

- name: Start the cluster
  hosts: spark
  gather_facts: no
  become: yes

  vars:
    ansible_python_interpreter: python3
    spark_home: /opt/spark
    data_dir: /data
    config_dir: /etc/spark

  tasks:
    - name: Wait for SSH connection
      wait_for_connection:

    - name: Gather facts, after SSH
      setup:

    # packer
    - name: Install docker-py
      pip:
        name: docker

    # packer
    - name: Render Docker bash script
      copy:
        src: templates/docker-bash.sh
        dest: /home/{{ ansible_user }}/docker-bash.sh
        owner: '{{ ansible_user }}'
        mode: u+x

    # packer
    - name: Automatically connect to container on login
      lineinfile:
        path: /home/{{ ansible_user }}/.bashrc
        line: source ./docker-bash.sh

    - name: Create directories
      file:
        path: '{{ item }}'
        state: directory
      with_items:
        - '{{ data_dir }}'
        - '{{ config_dir }}'

    - name: Render Spark config
      template:
        src: 'templates/{{ item }}'
        dest: '{{ config_dir }}/{{ item }}'
      with_items:
        - spark-defaults.conf
        - spark-env.sh
        - log4j.properties

    # TODO: Need to update this to new-style login, at some point?
    - name: Log in to ECR
      shell: $(aws ecr get-login --no-include-email --region {{ aws_region }})
      environment:
        AWS_ACCESS_KEY_ID: '{{ aws_access_key_id }}'
        AWS_SECRET_ACCESS_KEY: '{{ aws_secret_access_key }}'

    - name: Set master facts
      set_fact:
        container_name: spark-master
        runtime: '{{ master_docker_runtime }}'
        command: spark-class org.apache.spark.deploy.master.Master
      when: '"master" in group_names'

    - name: Set worker facts
      set_fact:
        container_name: spark-worker
        runtime: '{{ worker_docker_runtime }}'
        command: >
          spark-class org.apache.spark.deploy.worker.Worker
          spark://{{ master_private_ip }}:7077
      when: '"workers" in group_names'

    # TODO: Don't restart if image is unchanged.
    - name: Start containers
      docker_container:
        image: '{{ docker_image }}'
        name: '{{ container_name }}'
        command: '{{ command }}'
        runtime: '{{ runtime }}'
        state: started
        network_mode: host
        pull: true
        timeout: 300
        volumes:
          - '{{ config_dir }}:{{ spark_home }}/conf'
          - '{{ data_dir }}:{{ data_dir }}'
        env:
          AWS_ACCESS_KEY_ID: '{{ aws_access_key_id }}'
          AWS_SECRET_ACCESS_KEY: '{{ aws_secret_access_key }}'
          SPARK_ENV: prod