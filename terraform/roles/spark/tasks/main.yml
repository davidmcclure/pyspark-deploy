---

- name: Wait for SSH connection
  wait_for_connection:

- name: Gather facts, after SSH
  setup:

- name: Create config directory
  file:
    path: '{{ spark_config_dir }}'
    state: directory

- name: Render Spark config
  template:
    src: '{{ item }}.j2'
    dest: '{{ spark_config_dir }}/{{ item }}'
  with_items:
    - spark-defaults.conf
    - spark-env.sh
    - log4j.properties

# TODO: No need for docker-login, if we use `command` directly.
- name: Render scripts
  template:
    src: '{{ item }}.j2'
    dest: '/home/{{ ansible_user }}/{{ item }}'
    owner: '{{ ansible_user }}'
    mode: u+x
  with_items:
    # TODO: Something broken with docker-bash.sh
    - docker-bash.sh
    - docker-login.sh

- name: Automatically connect to container on login
  lineinfile:
    path: /home/{{ ansible_user }}/.bashrc
    line: source ./docker-bash.sh

# Super annoying issue regarding 'unattended-upgrades' on the AMI.  See
# https://github.com/ansible/ansible/issues/25414.  This can and does slow down
# deployment, unfortunately.  A more appropriate solution, given the simple use
# case of just needing awscli, would probably be to either run awscli locally
# and find a good way to pass the docker login token to Ansible, find another
# way to install awscli (that doesn't take even longer), or create an AMI with
# awscli already installed (and possibly with 'unattended-upgrades' disabled).
# - name: Wait for any possibly running unattended upgrade to finish
#   raw: systemd-run --property="After=apt-daily.service apt-daily-upgrade.service" --wait /bin/true

# TODO: Check for ecr_login boolean flag. Break into separate file.

# TODO: Do in Docker AMI build.
- name: Install awscli
  apt:
    name: awscli

# TODO: Provide login command directly, set ENV via Ansible?
- name: Login to AWS ECR
  command: sh /home/{{ ansible_user }}/docker-login.sh

- name: Start master
  include_tasks: start_container.yml
  when: '"master" in group_names'
  vars:
    name: spark-master
    command: spark-class org.apache.spark.deploy.master.Master

- name: Start workers
  include_tasks: start_container.yml
  when: '"workers" in group_names'
  vars:
    name: spark-worker
    command: >
      spark-class org.apache.spark.deploy.worker.Worker
      {{ spark_master_url }}
